import os, io, json, time, re, uuid, shutil, requests, cv2
from datetime import datetime
from pathlib import Path
from PIL import Image
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np

# =========================
# í™˜ê²½/ì„¤ì •
# =========================
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY")
DATA_ROOT       = Path(os.getenv("DATA_ROOT", "./data/users")).resolve()
OCR_ENDPOINT    = "https://api.upstage.ai/v1/document-digitization"
GPT_MODEL       = "gpt-4o-mini"   # í•„ìš”ì‹œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ êµì²´

assert UPSTAGE_API_KEY, "í™˜ê²½ë³€ìˆ˜ UPSTAGE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤(.env í™•ì¸)."
assert OPENAI_API_KEY,  "í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤(.env í™•ì¸)."

client = OpenAI(api_key=OPENAI_API_KEY)

DOC_TYPES = [
    "ë³´í—˜ê¸ˆì²­êµ¬ì„œ",
    "ì§„ë‹¨ì„œ",
    "ì…í‡´ì›í™•ì¸ì„œ",
    "ì²˜ë°©ì „",
    "ì‚¬ê³ ê²½ìœ„ì„œ",
    "ìˆ˜ë¦¬ê²¬ì ì„œ",
    "ì‚¬ë§ì§„ë‹¨ì„œ",
    "ê¸°íƒ€ ë¬¸ì„œ"
]

# =========================
# ìœ í‹¸
# =========================
def ensure_user_folder(customer_id: str) -> Path:
    """ê³ ê°ë³„/ì—…ë¡œë“œíšŒì°¨ë³„ í´ë” ìƒì„±"""
    safe_id = re.sub(r"[^0-9A-Za-zê°€-í£_-]", "_", customer_id.strip())
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder = DATA_ROOT / safe_id / ts
    folder.mkdir(parents=True, exist_ok=True)
    return folder

def save_uploaded_file(file, dst_path: Path) -> Path:
    with open(dst_path, "wb") as f:
        f.write(file.read())
    return dst_path

def preprocess_image_to_png(src_path: Path) -> Path:
    """
    ê°„ë‹¨ ì „ì²˜ë¦¬(ê·¸ë ˆì´/ì˜¤ì¸  ì´ì§„í™”) í›„ PNGë¡œ ì €ì¥ (ìœ ë‹ˆì½”ë“œ ê²½ë¡œ ì•ˆì „)
    - ëª¨ë“  í¬ë§·ì„ np.fromfile + cv2.imdecodeë¡œ ì½ìŒ
    - ì €ì¥ì€ cv2.imencode(...).tofile(...)ë¡œ ì²˜ë¦¬
    """
    # 1) ì½ê¸° (cv2.imread ëŒ€ì‹  imdecode ì‚¬ìš©: í•œê¸€ ê²½ë¡œ ì•ˆì „)
    data = np.fromfile(str(src_path), dtype=np.uint8)
    img = cv2.imdecode(data, cv2.IMREAD_COLOR)

    # PIL í´ë°±
    if img is None:
        try:
            pil = Image.open(src_path).convert("RGB")
            img = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)
        except Exception as e:
            raise RuntimeError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {src_path} ({e})")

    # 2) ì „ì²˜ë¦¬
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (3, 3), 0)
    _, thr = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # 3) ì €ì¥ (imwrite ëŒ€ì‹  imencode+tofile: í•œê¸€ ê²½ë¡œ ì•ˆì „)
    out = src_path.with_suffix(".proc.png")
    ok, buf = cv2.imencode(".png", thr)
    if not ok:
        raise RuntimeError("PNG ì¸ì½”ë”© ì‹¤íŒ¨")

    # ì‹¤ì œ ì“°ê¸°
    buf.tofile(str(out))

    # 4) ì¡´ì¬/í¬ê¸° í™•ì¸
    if not out.exists() or out.stat().st_size == 0:
        raise RuntimeError(f"ì „ì²˜ë¦¬ëœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {out}")

    return out

def run_upstage_ocr(image_path: Path) -> dict:
    """Upstage OCR í˜¸ì¶œ -> {text, ...}"""
    with open(image_path, "rb") as f:
        files = {"document": f}
        data = {"model": "ocr"}
        headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
        res = requests.post(OCR_ENDPOINT, headers=headers, files=files, data=data, timeout=60)
        res.raise_for_status()
        return res.json()

def classify_with_gpt(ocr_text: str) -> dict:
    """
    GPTë¡œ ë¬¸ì„œìœ í˜• ë¶„ë¥˜ + í•µì‹¬ í•„ë“œ ì¶”ì¶œ.
    JSONìœ¼ë¡œ ê°•ì œ ë°˜í™˜.
    """
    system_prompt = (
        "ë‹¹ì‹ ì€ ë³´í—˜ ë¬¸ì„œ ë¶„ë¥˜/ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìê°€ ì œê³µí•œ OCR í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜•ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”: "
        f"{', '.join(DOC_TYPES)}. "
        "ê°€ëŠ¥í•˜ë©´ í•µì‹¬ í•„ë“œ(ì´ë¦„, ìƒë…„ì›”ì¼, ë‚ ì§œ, ê¸ˆì•¡, ì¦ê¶Œ/ê³„ì•½ë²ˆí˜¸, ë³‘ì›/ê¸°ê´€ëª…, ê³„ì¢Œì •ë³´ ë“±)ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. "
        "ì •í™•ë„(confidence)ëŠ” 0~1 ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
    )
    user_prompt = f"""
[OCR_TEXT BEGIN]
{ocr_text[:20000]}
[OCR_TEXT END]

ë°˜í™˜ í˜•ì‹(JSON):
{{
  "doc_type": "<ìœ„ ëª©ë¡ ì¤‘ í•˜ë‚˜>",
  "confidence": 0.0,
  "key_fields": {{
      "name": "...",
      "dob": "...",
      "date": "...",
      "amount": "...",
      "policy_number": "...",
      "hospital": "...",
      "account": "..."
  }},
  "rationale": "ê°„ë‹¨ ê·¼ê±°"
}}
    """.strip()

    resp = client.chat.completions.create(
        model=GPT_MODEL,
        temperature=0.1,
        messages=[
            {"role":"system", "content": system_prompt},
            {"role":"user", "content": user_prompt}
        ]
    )
    content = resp.choices[0].message.content
    try:
        data = json.loads(content)
    except Exception:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „ ë˜í¼
        data = {"doc_type":"ê¸°íƒ€","confidence":0.0,"key_fields":{},"rationale":"parse_failed","raw":content}
    return data

# numpy import (ì§€ì—° ì‚¬ìš© ëŒ€ë¹„)
import numpy as np

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="ë³´í—˜ ì²­êµ¬ ì„œë¥˜ ì—…ë¡œë“œ Â· OCR Â· ë¶„ë¥˜", page_icon="ğŸ§¾", layout="wide")

st.title("ğŸ§¾ YMC ë³´í—˜ ì²­êµ¬")
st.caption("ë¹ ë¥¸ ì²­êµ¬ëŠ” YMC ë³´í—˜")

with st.sidebar:
    st.divider()
    st.subheader("ì´ë™ ë©”ë‰´")

    st.markdown(
        "<a href='http://localhost:8502/' target='_blank'>ğŸ—‚ï¸ ìƒˆ íƒ­ì—ì„œ ê´€ë¦¬ììš© ì—´ê¸°</a>",
        unsafe_allow_html=True
    )

with st.form("uploader", clear_on_submit=False):
    col1, col2 = st.columns([1,2])
    with col1:
        customer_id = st.text_input("ê³ ê° ID", placeholder="ì˜ˆ: í™ê¸¸ë™_31")
    with col2:
        uploads = st.file_uploader(
    "ë³´í—˜ ì²­êµ¬ ì„œë¥˜ ì—…ë¡œë“œ",
    type=["png","jpg","jpeg","pdf","tif","tiff","doc","docx","hwp","bmp"],
    accept_multiple_files=True
)
    submitted = st.form_submit_button("ì—…ë¡œë“œ ë° ë¶„ë¥˜ ì‹¤í–‰")

if submitted:
    # âœ… 0) ìœ íš¨ì„± ê²€ì‚¬
    if not uploads or len(uploads) == 0:
        st.warning("íŒŒì¼ì„ 1ê°œ ì´ìƒ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not customer_id.strip():
        st.warning("ê³ ê° ID ë˜ëŠ” ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # âœ… 1) ì…ë ¥ í˜•ì‹ ê²€ì¦ (ì´ë¦„+_ìˆ«ì í˜•ì‹ë§Œ í—ˆìš©)
    # ì˜ˆì‹œ: í™ê¸¸ë™_1, ê¹€ë¯¼ìˆ˜_23 ë“±
    if not re.match(r"^[ê°€-í£A-Za-z]+_[0-9]+$", customer_id.strip()):
        st.error("âŒ í˜•ì‹ ì˜¤ë¥˜: 'ì´ë¦„_ìˆ«ì' í˜•íƒœë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: í™ê¸¸ë™_31)")
        st.stop()

    # 2) ê³ ê° í´ë” ìƒì„±
    user_folder = ensure_user_folder(customer_id)

    results = []
    summary = []

    for uploaded in uploads:
        st.markdown(f"---\n**íŒŒì¼:** {uploaded.name}")
        original_path = user_folder / uploaded.name
        save_uploaded_file(uploaded, original_path)
        st.success(f"ì›ë³¸ ì €ì¥ ì™„ë£Œ: {original_path.name}")

        ext = original_path.suffix.lower()
        if ext in [".png",".jpg",".jpeg",".tif",".tiff",".bmp"]:
            proc = preprocess_image_to_png(original_path)
            targets = [proc]
        else:
            if ext in [".doc",".docx",".hwp"]:
                st.info("â„¹ï¸ .docx / .hwpëŠ” ì „ì²˜ë¦¬ ì—†ì´ OCR APIë¡œ ì „ì†¡í•©ë‹ˆë‹¤. ê°€ëŠ¥í•˜ë©´ PDF ê¶Œì¥.")
            targets = [original_path]

        ocr_texts = []
        for t in targets:
            try:
                with st.spinner(f"OCR ì¸ì‹ ì¤‘... ({t.name})"):
                    res = run_upstage_ocr(t)
                ocr_texts.append(res.get("text",""))
            except Exception as e:
                st.error(f"OCR ì‹¤íŒ¨: {e}")
                continue

        merged_text = "\n\n".join(ocr_texts).strip()

        with st.spinner("GPTë¡œ ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ ì¤‘..."):
            classify = classify_with_gpt(merged_text)

        stem = original_path.stem
        text_path = user_folder / f"{stem}.ocr_text.txt"
        json_path = user_folder / f"{stem}.classification.json"
        text_path.write_text(merged_text, encoding="utf-8")
        json_path.write_text(json.dumps(classify, ensure_ascii=False, indent=2), encoding="utf-8")

        results.append({
            "file": uploaded.name,
            "doc_type": classify.get("doc_type","ê¸°íƒ€ ë¬¸ì„œ"),
            "confidence": classify.get("confidence",0.0),
            "name": classify.get("key_fields",{}).get("name",""),
            "date": classify.get("key_fields",{}).get("date",""),
            "amount": classify.get("key_fields",{}).get("amount",""),
            "policy_number": classify.get("key_fields",{}).get("policy_number",""),
            "hospital": classify.get("key_fields",{}).get("hospital",""),
        })
        summary.append({
            "file": uploaded.name,
            "ocr_text_path": text_path.name,
            "classification_path": json_path.name,
            "classification": classify
        })

    # ğŸ”¸ íšŒì°¨ ìš”ì•½ ì €ì¥ (ê´€ë¦¬ì í™”ë©´ì´ ì½ìŒ)
    (user_folder / "summary.json").write_text(
        json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    try:
        from db import exec_tx
        import json
        from datetime import datetime

        exec_tx("""
            INSERT INTO documents
            (customer_id, uploaded_at, doc_type, confidence, key_fields,
            original_path, ocr_text_path, classification_json_path, source_ext)
            VALUES
            (:cid, :uploaded_at, :doc_type, :confidence, CAST(:key_fields AS JSON),
            :orig, :ocr, :cls, :ext)
            """, {
            "cid": customer_id,
            "uploaded_at": datetime.now(),
            "doc_type": classify.get("doc_type"),
            "confidence": float(classify.get("confidence") or 0),
            "key_fields": json.dumps(classify.get("key_fields") or {}, ensure_ascii=False),
            "orig": str(original_path),
            "ocr": str(text_path),
            "cls": str(json_path),
            "ext": original_path.suffix.lower().lstrip(".")
        })
        st.success("âœ… DBì— ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        st.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
